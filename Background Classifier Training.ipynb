{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b62b7d9",
   "metadata": {},
   "source": [
    "# Libraries used:\n",
    "\n",
    "numpy 1.19.5\n",
    "<br>\n",
    "tensorflow 2.4.1\n",
    "<br>\n",
    "pandas 1.2.4\n",
    "<br>\n",
    "matplotlib 3.3.4\n",
    "<br>\n",
    "tensorflow_addons 0.12.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26cddcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa2ad41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63444c2d",
   "metadata": {},
   "source": [
    "# Data processing and augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26d11ee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image_ID</th>\n",
       "      <th>class</th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymin</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID_007FAIEI</td>\n",
       "      <td>fruit_woodiness</td>\n",
       "      <td>87.0</td>\n",
       "      <td>87.5</td>\n",
       "      <td>228.0</td>\n",
       "      <td>311.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID_00G8K1V3</td>\n",
       "      <td>fruit_brownspot</td>\n",
       "      <td>97.5</td>\n",
       "      <td>17.5</td>\n",
       "      <td>245.0</td>\n",
       "      <td>354.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID_00WROUT9</td>\n",
       "      <td>fruit_brownspot</td>\n",
       "      <td>156.5</td>\n",
       "      <td>209.5</td>\n",
       "      <td>248.0</td>\n",
       "      <td>302.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID_00ZJEEK3</td>\n",
       "      <td>fruit_healthy</td>\n",
       "      <td>125.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>254.5</td>\n",
       "      <td>217.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID_018UIENR</td>\n",
       "      <td>fruit_brownspot</td>\n",
       "      <td>79.5</td>\n",
       "      <td>232.5</td>\n",
       "      <td>233.5</td>\n",
       "      <td>182.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Image_ID            class   xmin   ymin  width  height\n",
       "0  ID_007FAIEI  fruit_woodiness   87.0   87.5  228.0   311.0\n",
       "1  ID_00G8K1V3  fruit_brownspot   97.5   17.5  245.0   354.5\n",
       "2  ID_00WROUT9  fruit_brownspot  156.5  209.5  248.0   302.5\n",
       "3  ID_00ZJEEK3    fruit_healthy  125.0  193.0  254.5   217.0\n",
       "4  ID_018UIENR  fruit_brownspot   79.5  232.5  233.5   182.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('Train.csv')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91eef03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames=[]\n",
    "classes=[]\n",
    "bbox=[]\n",
    "for i in range(len(df)):\n",
    "    fnames.append('Train_Images\\\\{}.jpg'.format(df['Image_ID'][i]))\n",
    "    \n",
    "    classes.append(np.array([1,0]) )   \n",
    "    \n",
    "    bbox_coordinates=np.array([df['xmin'][i]+0.5*df['width'][i],df['ymin'][i]+0.5*df['height'][i],\n",
    "                               df['width'][i],df['height'][i]])\n",
    "    bbox.append(bbox_coordinates.astype('float32'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbcbd3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_p=fnames[0]\n",
    "fnames_sorted=[]\n",
    "fnames_count=[]\n",
    "count=0\n",
    "for i in range(1,len(fnames)):\n",
    "    count=count+1\n",
    "    fname_i=fnames[i]\n",
    "    if fname_i!=fname_p:\n",
    "        fnames_sorted.append(fname_p)\n",
    "        fname_p=fname_i\n",
    "        fnames_count.append(count)\n",
    "        count=0\n",
    "\n",
    "fnames_appearing_once=np.array(fnames_sorted)[np.where(np.array(fnames_count)==1)[0]]\n",
    "fnames_appearing_once_indexes=[]\n",
    "for i in range(len(fnames_appearing_once)):\n",
    "    fnames_appearing_once_indexes.append(fnames.index(fnames_appearing_once[i]))\n",
    "    \n",
    "    \n",
    "bboxes_once=[]\n",
    "classes_once=[]\n",
    "for index in np.array(fnames_appearing_once_indexes,dtype='int'):\n",
    "    bboxes_once.append(bbox[index])\n",
    "    classes_once.append(classes[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d009e0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_size=len(fnames_appearing_once)\n",
    "\n",
    "fnames_dataset = tf.data.Dataset.from_tensor_slices(fnames_appearing_once)\n",
    "classes_dataset = tf.data.Dataset.from_tensor_slices(classes_once)\n",
    "bbox_dataset = tf.data.Dataset.from_tensor_slices(bboxes_once)\n",
    "\n",
    "ds = tf.data.Dataset.zip((fnames_dataset, classes_dataset, bbox_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "670b7066",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.80\n",
    "ds_train1=ds.take(int(ds_size*train_ratio))\n",
    "ds_test1=ds.skip(int(ds_size*train_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06e69c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#background data\n",
    "@tf.function\n",
    "def map_fn(fn, arrays, dtype=tf.float32):\n",
    "    indices = tf.range(tf.shape(arrays[0])[0])\n",
    "    out = tf.map_fn(lambda ii: fn(*[array[ii] for array in arrays]), indices, \n",
    "                    fn_output_signature=dtype)\n",
    "    return out\n",
    "\n",
    "@tf.function\n",
    "def IoU(inputs):\n",
    "    anchor=inputs[:,:4]\n",
    "    target=inputs[:,4:]\n",
    "    \n",
    "    gl = tfa.losses.GIoULoss(mode = 'iou') \n",
    "    \n",
    "    boxes1 = tf.stack([anchor[:,1]-0.5*anchor[:,3], anchor[:,0]-0.5*anchor[:,2], \n",
    "                          anchor[:,1]+0.5*anchor[:,3], anchor[:,0]+0.5*anchor[:,2]],axis=-1)\n",
    "    boxes2 = tf.stack([target[:,1]-0.5*target[:,3], target[:,0]-0.5*target[:,2], \n",
    "                          target[:,1]+0.5*target[:,3], target[:,0]+0.5*target[:,2]],axis=-1)\n",
    "    loss = map_fn(gl, (boxes1, boxes2))\n",
    "    return 1-loss\n",
    "\n",
    "def background_data(bbox):\n",
    "    corner_bbox1=tf.stack([0.5*bbox[2], 0.5*bbox[3], bbox[2], bbox[3]], axis=-1)\n",
    "    corner_bbox2=tf.stack([0.5*bbox[2], 512.-0.5*bbox[3], bbox[2], bbox[3]], axis=-1)\n",
    "    corner_bbox3=tf.stack([512.-0.5*bbox[2], 0.5*bbox[3], bbox[2], bbox[3]], axis=-1)\n",
    "    corner_bbox4=tf.stack([512.-0.5*bbox[2], 512.-0.5*bbox[3], bbox[2], bbox[3]], axis=-1)\n",
    "    \n",
    "    corner_bboxes=tf.stack([corner_bbox1,corner_bbox2,corner_bbox3,corner_bbox4], axis=0)\n",
    "    \n",
    "    bboxes_original=tf.tile(tf.expand_dims(bbox,axis=0), tf.constant([4,1]))\n",
    "    \n",
    "    IoU_list=IoU(tf.concat([bboxes_original,corner_bboxes], axis=-1))\n",
    "    \n",
    "    background_bbox=tf.gather(corner_bboxes,tf.squeeze(tf.where(IoU_list<0.1),axis=-1),axis=0)\n",
    "    \n",
    "    return tf.unstack(background_bbox,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3faf6c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6138\n",
      "1544\n"
     ]
    }
   ],
   "source": [
    "bbox_bg_train=[]\n",
    "fnames_bg_train=[]\n",
    "classes_bg_train=[]\n",
    "\n",
    "for fnames, classes,bbox in ds_train1:\n",
    "    bbox_bg_train.extend([bbox])\n",
    "    fnames_bg_train.extend([fnames])\n",
    "    classes_bg_train.extend([classes])\n",
    "    background_bboxes=background_data(bbox)\n",
    "    if len(background_bboxes)!=0:\n",
    "        bbox_bg_train.extend(background_bboxes)\n",
    "        fnames_bg_train.extend(tf.unstack(tf.repeat(fnames,len(background_bboxes))))\n",
    "        classes_bg_train.extend(tf.unstack(tf.repeat(tf.constant([[0,1]]),len(background_bboxes), axis=0)))\n",
    "        \n",
    "print(len(fnames_bg_train))            \n",
    "fnames_bg_train_dataset = tf.data.Dataset.from_tensor_slices(fnames_bg_train)\n",
    "classes_bg_train_dataset = tf.data.Dataset.from_tensor_slices(classes_bg_train)\n",
    "bbox_bg_train_dataset = tf.data.Dataset.from_tensor_slices(bbox_bg_train)    \n",
    "\n",
    "ds_train1 = tf.data.Dataset.zip((fnames_bg_train_dataset, classes_bg_train_dataset, \n",
    "                                   bbox_bg_train_dataset))\n",
    "\n",
    "\n",
    "bbox_bg_test=[]\n",
    "fnames_bg_test=[]\n",
    "classes_bg_test=[]\n",
    "for fnames, classes,bbox in ds_test1:\n",
    "    bbox_bg_test.extend([bbox])\n",
    "    fnames_bg_test.extend([fnames])\n",
    "    classes_bg_test.extend([classes])\n",
    "    background_bboxes=background_data(bbox)\n",
    "    if len(background_bboxes)!=0:\n",
    "        bbox_bg_test.extend(background_bboxes)\n",
    "        fnames_bg_test.extend(tf.unstack(tf.repeat(fnames,len(background_bboxes))))\n",
    "        classes_bg_test.extend(tf.unstack(tf.repeat(tf.constant([[0,1]]),len(background_bboxes), axis=0)))\n",
    "\n",
    "print(len(fnames_bg_test))          \n",
    "fnames_bg_test_dataset = tf.data.Dataset.from_tensor_slices(fnames_bg_test)\n",
    "classes_bg_test_dataset = tf.data.Dataset.from_tensor_slices(classes_bg_test)\n",
    "bbox_bg_test_dataset = tf.data.Dataset.from_tensor_slices(bbox_bg_test)    \n",
    "\n",
    "ds_test1 = tf.data.Dataset.zip((fnames_bg_test_dataset, classes_bg_test_dataset, \n",
    "                                   bbox_bg_test_dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a4bcd4",
   "metadata": {},
   "source": [
    "Augmentation: rotate images and bounding boxes by 90, 180 and 270 degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2b2114a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbox_rotation(bbox,deg):\n",
    "    bbox=bbox.numpy()\n",
    "    #deg 1: 90, 2:180, 3: 270\n",
    "    if deg==0:\n",
    "        return tf.constant(bbox)\n",
    "    if deg==3:\n",
    "        return tf.constant([512-bbox[1],bbox[0],bbox[3],bbox[2]],dtype='float32')\n",
    "    \n",
    "    elif deg==2:\n",
    "        return tf.constant([512-bbox[0],512-bbox[1],bbox[2],bbox[3]], dtype='float32')\n",
    "\n",
    "    elif deg==1:\n",
    "        return tf.constant([bbox[1],512-bbox[0],bbox[3],bbox[2]], dtype='float32')\n",
    "    else:\n",
    "        return NotImplemented\n",
    "\n",
    "\n",
    "\n",
    "IMG_SIZE=512\n",
    "def process_img(img, deg):\n",
    "    image = tf.io.read_file(img)\n",
    "    image = tf.image.decode_jpeg(image, channels=3) \n",
    "    image = tf.image.convert_image_dtype(image, tf.float32) \n",
    "    image = tf.image.resize(image, [IMG_SIZE, IMG_SIZE])\n",
    "    image=tfa.image.rotate(image,tf.constant(np.pi*0.5*deg))\n",
    "    return image\n",
    "\n",
    "def read_images(fnames, classes, bbox, deg):\n",
    "    img = process_img(fnames, deg)\n",
    "    bbox=bbox_rotation(bbox,deg)\n",
    "    return img, classes, bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "384b6b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train=ds_train1.map(lambda img, classes, bbox: tf.py_function(func=read_images,\n",
    "          inp=[img, classes, bbox, 0.], Tout=(tf.float32,tf.int32,tf.float32)),\n",
    "          num_parallel_calls=tf.data.AUTOTUNE,\n",
    "          deterministic=True)\n",
    "\n",
    "ds_train_rotate1=ds_train1.map(lambda img, classes, bbox: tf.py_function(func=read_images,\n",
    "          inp=[img, classes, bbox, 1.], Tout=(tf.float32,tf.int32,tf.float32)),\n",
    "          num_parallel_calls=tf.data.AUTOTUNE,\n",
    "          deterministic=True)\n",
    "\n",
    "ds_train_rotate2=ds_train1.map(lambda img, classes, bbox: tf.py_function(func=read_images,\n",
    "          inp=[img, classes, bbox, 2.], Tout=(tf.float32,tf.int32,tf.float32)),\n",
    "          num_parallel_calls=tf.data.AUTOTUNE,\n",
    "          deterministic=True)\n",
    "\n",
    "ds_train_rotate3=ds_train1.map(lambda img, classes, bbox: tf.py_function(func=read_images,\n",
    "          inp=[img, classes, bbox, 3.], Tout=(tf.float32,tf.int32,tf.float32)),\n",
    "          num_parallel_calls=tf.data.AUTOTUNE,\n",
    "          deterministic=True)\n",
    "\n",
    "\n",
    "ds_train=ds_train.concatenate(ds_train_rotate1)\n",
    "ds_train=ds_train.concatenate(ds_train_rotate2)\n",
    "ds_train=ds_train.concatenate(ds_train_rotate3)\n",
    "\n",
    "ds_test=ds_test1.map(lambda img, classes, bbox: tf.py_function(func=read_images,\n",
    "          inp=[img, classes, bbox, 0.], Tout=(tf.float32,tf.int32,tf.float32)),\n",
    "          num_parallel_calls=tf.data.AUTOTUNE,\n",
    "          deterministic=True)\n",
    "\n",
    "ds_test_rotate1=ds_test1.map(lambda img, classes, bbox: tf.py_function(func=read_images,\n",
    "          inp=[img, classes, bbox, 1.], Tout=(tf.float32,tf.int32,tf.float32)),\n",
    "          num_parallel_calls=tf.data.AUTOTUNE,\n",
    "          deterministic=True)\n",
    "\n",
    "ds_test_rotate2=ds_test1.map(lambda img, classes, bbox: tf.py_function(func=read_images,\n",
    "          inp=[img, classes, bbox, 2.], Tout=(tf.float32,tf.int32,tf.float32)),\n",
    "          num_parallel_calls=tf.data.AUTOTUNE,\n",
    "          deterministic=True)\n",
    "\n",
    "ds_test_rotate3=ds_test1.map(lambda img, classes, bbox: tf.py_function(func=read_images,\n",
    "          inp=[img, classes, bbox, 3.], Tout=(tf.float32,tf.int32,tf.float32)),\n",
    "          num_parallel_calls=tf.data.AUTOTUNE,\n",
    "          deterministic=True)\n",
    "\n",
    "ds_test=ds_test.concatenate(ds_test_rotate1)\n",
    "ds_test=ds_test.concatenate(ds_test_rotate2)\n",
    "ds_test=ds_test.concatenate(ds_test_rotate3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67355df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train_final = ds_train.map(lambda a, b, c: (a, (b,c)))\n",
    "ds_test_final = ds_test.map(lambda a, b, c: (a, (b,c)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267f52ae",
   "metadata": {},
   "source": [
    "# Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9cf546d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def BGCN_training_loss(class_pred, class_t):\n",
    "    \n",
    "    cce = tf.keras.losses.CategoricalCrossentropy()\n",
    "    class_loss=cce(class_t, class_pred)\n",
    "    \n",
    "    return class_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b1a219",
   "metadata": {},
   "source": [
    "## Background Classifier network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96f44a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_bg = tf.keras.applications.VGG16(\n",
    "                weights='imagenet', \n",
    "                input_shape=(64, 64, 3), \n",
    "                include_top=False)\n",
    "\n",
    "#conv1=tf.keras.layers.Conv2D(512, (3,3), activation='relu',padding='same')(base_model.layers[-6].output)\n",
    "\n",
    "flatten1_bg=tf.keras.layers.Flatten()(base_model_bg.output)\n",
    "#bn1=tf.keras.layers.BatchNormalization()(flatten1)\n",
    "dense1_bg=tf.keras.layers.Dense(1024, activation='relu')(flatten1_bg)\n",
    "bn2_bg=tf.keras.layers.BatchNormalization()(dense1_bg)\n",
    "dense2_bg=tf.keras.layers.Dense(512, activation='relu')(bn2_bg)\n",
    "       \n",
    "box_cls_bg=tf.keras.layers.Dense(2, activation='softmax')(dense2_bg)\n",
    "\n",
    "bgcn_model=tf.keras.models.Model(inputs=base_model_bg.inputs, outputs=box_cls_bg)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33cb85b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BGCN(tf.keras.models.Model):\n",
    "    def __init__(self):\n",
    "        super(BGCN, self).__init__()\n",
    "                 \n",
    "\n",
    "        self.base_model = bgcn_model \n",
    "        \n",
    "        #self.base_model.trainable=False\n",
    "            \n",
    "        for i in range(14):\n",
    "            self.base_model.layers[i].trainable=False\n",
    "\n",
    "    def call(self, image):\n",
    "        image=tf.reverse(image, axis=[-1])\n",
    "        image=tf.image.per_image_standardization(image)\n",
    "        \n",
    "        x = self.base_model(image, training=False)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    @tf.function\n",
    "    def prediction(self, image_input, bbox):\n",
    "        bbox_cropped_coordinates=tf.stack([bbox[:,1]-0.5*bbox[:,3], \n",
    "                                                bbox[:,0]-0.5*bbox[:,2], \n",
    "                          bbox[:,1]+0.5*bbox[:,3], bbox[:,0]\n",
    "                                                +0.5*bbox[:,2]],axis=-1)/512\n",
    "        \n",
    "        img_cropped=tf.image.crop_and_resize(\n",
    "            image_input, bbox_cropped_coordinates, \n",
    "            box_indices=tf.range(0, tf.shape(bbox)[0], 1), crop_size=[64,64])\n",
    "        \n",
    "        \n",
    "        image=tf.reverse(img_cropped, axis=[-1])\n",
    "        image=tf.image.per_image_standardization(image)\n",
    "        \n",
    "        x = self.base_model(image, training=False)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    #@tf.function\n",
    "    def train_step(self, data):\n",
    "        \n",
    "        inputs= data[0]\n",
    "        targets= data[1]\n",
    "        \n",
    "        classes=targets[0]\n",
    "        bbox=targets[1]\n",
    "        \n",
    "        bbox_cropped_coordinates=tf.stack([bbox[:,1]-0.5*bbox[:,3], \n",
    "                                                bbox[:,0]-0.5*bbox[:,2], \n",
    "                          bbox[:,1]+0.5*bbox[:,3], bbox[:,0]\n",
    "                                                +0.5*bbox[:,2]],axis=-1)/512\n",
    "        \n",
    "        img_cropped=tf.image.crop_and_resize(\n",
    "            inputs, bbox_cropped_coordinates, \n",
    "            box_indices=tf.range(0, tf.shape(bbox)[0], 1), crop_size=[64,64])\n",
    "        \n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            class_pred1 = self(img_cropped)\n",
    "            \n",
    "            loss_value = BGCN_training_loss(class_pred1, classes)\n",
    "        \n",
    "        grads = tape.gradient(loss_value, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "        self.compiled_metrics.update_state(classes, class_pred1)\n",
    "       \n",
    "        return {m.name: m.result() for m in self.metrics}#{'loss_value': loss_value} \n",
    "    \n",
    "    \n",
    "    def test_step(self, data):\n",
    "        inputs= data[0]\n",
    "        targets= data[1]\n",
    "        \n",
    "        classes=targets[0]\n",
    "        bbox=targets[1]\n",
    "        \n",
    "        bbox_cropped_coordinates=tf.stack([bbox[:,1]-0.5*bbox[:,3], \n",
    "                                                bbox[:,0]-0.5*bbox[:,2], \n",
    "                          bbox[:,1]+0.5*bbox[:,3], bbox[:,0]\n",
    "                                                +0.5*bbox[:,2]],axis=-1)/512\n",
    "        \n",
    "        img_cropped=tf.image.crop_and_resize(\n",
    "            inputs, bbox_cropped_coordinates, \n",
    "            box_indices=tf.range(0, tf.shape(bbox)[0], 1), crop_size=[64,64])\n",
    "        \n",
    "        \n",
    "        class_pred1 = self(img_cropped)\n",
    "        \n",
    "        self.compiled_metrics.update_state(classes, class_pred1)\n",
    "        \n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b098226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 64, 64, 3)]       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 64, 64, 64)        1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 64, 64, 64)        36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 32, 32, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 32, 32, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 16, 16, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 16, 16, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 16, 16, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 8, 8, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1024)              2098176   \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 1026      \n",
      "=================================================================\n",
      "Total params: 17,342,786\n",
      "Trainable params: 9,705,474\n",
      "Non-trainable params: 7,637,312\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bgcn1=BGCN()\n",
    "inputs = tf.keras.Input(shape=(64, 64, 3))\n",
    "bgcn1(inputs)\n",
    "\n",
    "bgcn1.base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5265345",
   "metadata": {},
   "source": [
    "# Training:\n",
    "To train the classifier, uncomment the following block. This will reproduce and save the weight files in the 'checkpoints' folder. If the cell is not run, the weights will be loaded from the 'checkpoints' folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3626dc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_batched = ds_train_final.batch(100, drop_remainder=True)\n",
    "# ds_batched_test = ds_test_final.batch(100, drop_remainder=True)\n",
    "\n",
    "# bgcn1=BGCN()\n",
    "# bgcn1.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss=None,\n",
    "#            metrics=[tf.keras.metrics.CategoricalAccuracy()])\n",
    "\n",
    "\n",
    "# bgcn1.fit(x=ds_batched,  shuffle=True, epochs=1, \n",
    "#     validation_data=ds_batched_test, validation_steps=50)\n",
    "\n",
    "\n",
    "# bgcn1.save_weights('./checkpoints/BGCN_9600_mini_batches_adam_0.0001_1epoch_trainable_vgg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99a5c4a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x2f5cb1240a0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_batched = ds_train_final.batch(100, drop_remainder=True)\n",
    "ds_batched_test = ds_test_final.batch(100, drop_remainder=True)\n",
    "\n",
    "bgcn1=BGCN()\n",
    "bgcn1.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss=None,\n",
    "           metrics=[tf.keras.metrics.CategoricalAccuracy()])\n",
    "bgcn1.load_weights('./checkpoints/BGCN_9600_mini_batches_adam_0.0001_1epoch_trainable_vgg') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
